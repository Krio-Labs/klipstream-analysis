# Enhanced GPU Optimization Configuration
# Environment variables for comprehensive GPU optimization features

# =============================================================================
# AUTOMATIC MIXED PRECISION (AMP) CONFIGURATION
# =============================================================================

# Enable/disable Automatic Mixed Precision
# Provides significant speedup on Tensor Core GPUs (V100, A100, L4, etc.)
# Default: true (auto-detected based on GPU capabilities)
ENABLE_AMP=true

# AMP data type (float16 or bfloat16)
# float16: Better compatibility, slightly lower precision
# bfloat16: Better numerical stability, requires newer GPUs
# Default: float16
AMP_DTYPE=float16

# =============================================================================
# MEMORY OPTIMIZATION CONFIGURATION
# =============================================================================

# Enable comprehensive memory optimization
# Includes adaptive cleanup, fragmentation handling, and memory monitoring
# Default: true
ENABLE_MEMORY_OPTIMIZATION=true

# Memory cleanup threshold (0.0 to 1.0)
# Triggers cleanup when GPU memory usage exceeds this percentage
# Default: 0.8 (80%)
MEMORY_CLEANUP_THRESHOLD=0.8

# Maximum allowed memory fragmentation (0.0 to 1.0)
# Warns when fragmentation exceeds this percentage
# Default: 0.3 (30%)
MAX_MEMORY_FRAGMENTATION=0.3

# Memory monitoring interval in seconds
# How often to check memory usage during processing
# Default: 1.0
MEMORY_MONITORING_INTERVAL=1.0

# =============================================================================
# PARALLEL PROCESSING CONFIGURATION
# =============================================================================

# Enable parallel audio chunking
# Uses ThreadPoolExecutor for concurrent chunk creation
# Default: true
ENABLE_PARALLEL_CHUNKING=true

# Maximum number of chunk workers
# Auto-detected based on CPU cores, but can be overridden
# Default: min(cpu_count, 8)
MAX_CHUNK_WORKERS=8

# I/O throttle delay in seconds
# Prevents overwhelming storage with concurrent I/O operations
# Default: 0.01 (10ms)
CHUNK_IO_THROTTLE_DELAY=0.01

# =============================================================================
# DEVICE-SPECIFIC OPTIMIZATIONS
# =============================================================================

# Enable device-specific optimizations
# Automatically configures optimal settings for CUDA, MPS, or CPU
# Default: true
ENABLE_DEVICE_OPTIMIZATION=true

# CUDA-specific optimizations
# Enable cuDNN benchmark for consistent input sizes
# Default: true
CUDA_BENCHMARK=true

# Enable TF32 on Ampere+ GPUs (RTX 30/40 series, A100, H100, L4)
# Provides speedup with minimal accuracy loss
# Default: true
CUDA_TF32=true

# CPU thread configuration
# Number of threads for CPU inference (auto-detected if not set)
# Default: min(cpu_count, 8)
CPU_THREADS=8

# CPU interop threads
# Number of threads for inter-operation parallelism
# Default: min(cpu_count // 2, 4)
CPU_INTEROP_THREADS=4

# =============================================================================
# PERFORMANCE MONITORING CONFIGURATION
# =============================================================================

# Enable comprehensive performance monitoring
# Tracks memory usage, processing times, and optimization effectiveness
# Default: true
ENABLE_PERFORMANCE_MONITORING=true

# Save performance metrics to file
# Creates detailed performance reports for analysis
# Default: true
SAVE_PERFORMANCE_METRICS=true

# Performance metrics file path
# Where to save performance data
# Default: /tmp/transcription_performance_metrics.json
PERFORMANCE_METRICS_FILE=/tmp/transcription_performance_metrics.json

# =============================================================================
# DEBUGGING AND LOGGING CONFIGURATION
# =============================================================================

# Enable debug logging
# Provides detailed information about optimization decisions
# Default: false (to reduce log noise)
ENABLE_DEBUG_LOGGING=false

# Log optimization status on startup
# Shows which optimizations are enabled/disabled
# Default: true
LOG_OPTIMIZATION_STATUS=true

# =============================================================================
# BATCH PROCESSING CONFIGURATION
# =============================================================================

# Override automatic batch size detection
# Leave empty for auto-detection based on GPU memory
# Default: auto-detected
# GPU_BATCH_SIZE=8

# Minimum batch size
# Fallback when auto-detection fails
# Default: 1
MIN_BATCH_SIZE=1

# Maximum batch size
# Upper limit for batch size to prevent OOM
# Default: 16
MAX_BATCH_SIZE=16

# =============================================================================
# AUDIO CHUNKING CONFIGURATION
# =============================================================================

# Override chunk duration (in minutes)
# Leave empty for auto-detection based on GPU memory
# Default: auto-detected (8-15 minutes based on GPU)
# CHUNK_DURATION_MINUTES=10

# Chunk overlap (in seconds)
# Overlap between consecutive chunks for better accuracy
# Default: auto-detected (8-15 seconds based on chunk duration)
# CHUNK_OVERLAP_SECONDS=10

# =============================================================================
# CLOUD RUN SPECIFIC CONFIGURATION
# =============================================================================

# Enable Cloud Run optimizations
# Adjusts settings for Cloud Run environment constraints
# Default: auto-detected
CLOUD_RUN_ENVIRONMENT=false

# Cloud Run memory limit (in GB)
# Used for optimization calculations
# Default: 32 (Cloud Run GPU instance default)
CLOUD_RUN_MEMORY_GB=32

# Cloud Run timeout (in seconds)
# Maximum processing time allowed
# Default: 3600 (1 hour)
CLOUD_RUN_TIMEOUT_SECONDS=3600

# =============================================================================
# FALLBACK CONFIGURATION
# =============================================================================

# Enable fallback to CPU when GPU fails
# Automatically switches to CPU processing on GPU errors
# Default: true
ENABLE_GPU_FALLBACK=true

# Enable fallback to original transcriber
# Falls back to non-optimized version if enhanced version fails
# Default: true
ENABLE_ENHANCED_FALLBACK=true

# Fallback retry attempts
# Number of times to retry failed operations
# Default: 3
FALLBACK_RETRY_ATTEMPTS=3

# =============================================================================
# EXPERIMENTAL FEATURES
# =============================================================================

# Enable experimental torch.compile optimization
# May provide additional speedup on newer PyTorch versions
# Default: false (experimental)
ENABLE_TORCH_COMPILE=false

# Torch compile mode
# Options: default, reduce-overhead, max-autotune
# Default: reduce-overhead
TORCH_COMPILE_MODE=reduce-overhead

# Enable experimental memory mapping
# Uses memory mapping for large audio files
# Default: false (experimental)
ENABLE_MEMORY_MAPPING=false

# =============================================================================
# COMPATIBILITY CONFIGURATION
# =============================================================================

# Minimum CUDA compute capability required for optimizations
# Default: 7.0 (for Tensor Core support)
MIN_CUDA_COMPUTE_CAPABILITY=7.0

# Minimum GPU memory required (in GB)
# Default: 4.0
MIN_GPU_MEMORY_GB=4.0

# Enable compatibility mode for older GPUs
# Disables advanced features for better compatibility
# Default: false
ENABLE_COMPATIBILITY_MODE=false

# =============================================================================
# USAGE EXAMPLES
# =============================================================================

# Example 1: Maximum performance on high-end GPU (A100, H100, L4)
# ENABLE_AMP=true
# ENABLE_MEMORY_OPTIMIZATION=true
# ENABLE_PARALLEL_CHUNKING=true
# ENABLE_DEVICE_OPTIMIZATION=true
# CUDA_TF32=true
# MAX_CHUNK_WORKERS=8
# GPU_BATCH_SIZE=16

# Example 2: Conservative settings for older GPU (GTX 1080, RTX 2080)
# ENABLE_AMP=false
# ENABLE_MEMORY_OPTIMIZATION=true
# ENABLE_PARALLEL_CHUNKING=false
# CUDA_TF32=false
# GPU_BATCH_SIZE=4

# Example 3: CPU-only deployment
# ENABLE_AMP=false
# ENABLE_DEVICE_OPTIMIZATION=true
# ENABLE_PARALLEL_CHUNKING=true
# CPU_THREADS=8
# GPU_BATCH_SIZE=1

# Example 4: Cloud Run deployment
# ENABLE_AMP=true
# ENABLE_MEMORY_OPTIMIZATION=true
# ENABLE_PARALLEL_CHUNKING=true
# CLOUD_RUN_ENVIRONMENT=true
# CLOUD_RUN_MEMORY_GB=32
# SAVE_PERFORMANCE_METRICS=true

# =============================================================================
# NOTES
# =============================================================================

# 1. AMP requires CUDA compute capability 7.0+ for optimal performance
# 2. TF32 is only available on Ampere+ GPUs (RTX 30/40 series, A100, H100, L4)
# 3. Parallel chunking benefits from fast storage (SSD recommended)
# 4. Memory optimization is crucial for long audio files (>1 hour)
# 5. Performance monitoring adds minimal overhead but provides valuable insights
# 6. Debug logging can be verbose - use only for troubleshooting
# 7. Batch size auto-detection works well in most cases
# 8. Cloud Run settings are optimized for the standard GPU instance configuration
